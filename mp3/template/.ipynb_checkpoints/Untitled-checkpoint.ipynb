{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from collections import Counter\n",
    "import pickle\n",
    "import sys\n",
    "import argparse\n",
    "import configparser\n",
    "import copy\n",
    "import numpy as np\n",
    "from os import listdir\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6000/6000 [00:31<00:00, 193.45it/s]\n",
      "100%|██████████| 2000/2000 [00:09<00:00, 204.64it/s]\n",
      "100%|██████████| 4000/4000 [00:22<00:00, 177.82it/s]\n",
      "100%|██████████| 1000/1000 [00:05<00:00, 167.83it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_set, train_labels, dev_set, dev_labels = load_dataset(r\"C:\\Users\\Michal\\Desktop\\UIUC\\CS_440-ECE_448\\MP3_data_zip\\train\",r\"C:\\Users\\Michal\\Desktop\\UIUC\\CS_440-ECE_448\\MP3_data_zip\\dev\",False,True)\n",
    "pickle.dump(train_set, open(\"train_set.p\", \"wb\" ))\n",
    "pickle.dump(train_labels, open(\"train_labels.p\", \"wb\" ))\n",
    "pickle.dump(dev_set, open(\"dev_set.p\", \"wb\" ))\n",
    "pickle.dump(dev_labels, open(\"dev_labels.p\", \"wb\" ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = pickle.load( open(\"train_set.p\", \"rb\"))\n",
    "train_labels = pickle.load(open(\"train_labels.p\", \"rb\"))\n",
    "dev_set = pickle.load(open(\"dev_set.p\", \"rb\"))\n",
    "dev_labels = pickle.load(open(\"dev_labels.p\", \"rb\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'keys'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-40-6857640cea48>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_labels\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdev_set\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdev_labels\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0.6\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-37-07a255b4063f>\u001b[0m in \u001b[0;36mmain\u001b[1;34m(train_set, train_label, dev_set, dev_label, stemming, lower_case, laplace, pos_prior)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_label\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdev_set\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdev_label\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mstemming\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlower_case\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlaplace\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpos_prior\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m     \u001b[0mpredicted_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnaiveBayes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdev_set\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlaplace\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpos_prior\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[0maccuracy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprecision\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecall\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompute_accuracies\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredicted_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdev_set\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdev_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-39-8201f47a023c>\u001b[0m in \u001b[0;36mnaiveBayes\u001b[1;34m(train_set, train_labels, dev_set, smoothing_parameter, pos_prior)\u001b[0m\n\u001b[0;32m     41\u001b[0m         \u001b[0mprob_pos\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpos_prior\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m         \u001b[0mprob_neg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mpos_prior\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m         \u001b[0mrev_total_words\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcur_review\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mcur_key\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrev_count\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'keys'"
     ]
    }
   ],
   "source": [
    "main(train_set,train_labels,dev_set,dev_labels,False,True,1,0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naiveBayes(train_set, train_labels, dev_set, smoothing_parameter, pos_prior):\n",
    "    \n",
    "\n",
    "\n",
    "    # TODO: Write your code here\n",
    "\n",
    "    def laplace_smooth(count, k, N, k_x):\n",
    "        return math.log( (count + k) / (N + k_x) )\n",
    "\n",
    "    # if pos_prior == 1:\n",
    "    #     return [1 for i in range(len(dev_set))]\n",
    "    # elif pos_prior == 0:\n",
    "    #     return [0 for i in range(len(dev_set))]\n",
    "    \n",
    "    #word_map = {}\n",
    "\n",
    "    list_dev = []\n",
    "\n",
    "    pos_counter = Counter()\n",
    "    neg_counter = Counter()\n",
    "    pos_word_count = 0\n",
    "    neg_word_count = 0\n",
    "    # go through all reviews in training set\n",
    "    for i in range(len(train_set)):\n",
    "        cur_list = train_set[i].copy()\n",
    "        #word_inner_list.clear()\n",
    "        # go through each word in the review\n",
    "        for j in range(len(cur_list) - 1):\n",
    "            if(train_labels[i]): # check if positive review\n",
    "                pos_counter.update(cur_list[j]) \n",
    "                pos_word_count += 1\n",
    "            else:\n",
    "                neg_counter.update(cur_list[j])\n",
    "                neg_word_count += 1\n",
    "\n",
    "    #counter_dev = Counter()\n",
    "\n",
    "\n",
    "    for cur_review in dev_set:\n",
    "        rev_count = Counter(cur_review)\n",
    "        prob_pos = math.log(pos_prior)\n",
    "        prob_neg = math.log(1 - pos_prior)\n",
    "        rev_total_words = len( list(rev_count.keys() ))\n",
    "\n",
    "        for cur_key in rev_count:\n",
    "            #prob_word_pos = math.log( pos_counter.get(cur_key,0) / pos_word_count )\n",
    "            prob_laplace_smooth_pos = ( laplace_smooth(pos_counter.get(cur_key,0)\n",
    "                ,smoothing_parameter, len( list(pos_count.keys()) ) ) )\n",
    "\n",
    "            #prob_word_neg = math.log( neg_counter.get(cur_key,0) / neg_word_count )\n",
    "            prob_laplace_smooth_neg = ( laplace_smooth(neg_counter.get(cur_key,0)\n",
    "                ,smoothing_parameter, len( list( neg_count.keys() ) ) ) )\n",
    "                \n",
    "            prob_pos += prob_laplace_smooth_pos\n",
    "            prob_neg += prob_laplace_smooth_neg\n",
    "\n",
    "        if(prob_pos > prob_neg):\n",
    "            list_dev.append(1)\n",
    "        else:\n",
    "            list_dev.append(0)\n",
    "\n",
    "\n",
    "\n",
    "    # return predicted labels of development set (make sure it's a list, not a numpy array or similar)\n",
    "    return list_dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracies(predicted_labels, dev_set, dev_labels):\n",
    "    yhats = predicted_labels\n",
    "    accuracy = np.mean(yhats == dev_labels)\n",
    "    tp = np.sum([yhats[i] == dev_labels[i] and yhats[i] == 1 for i in range(len(yhats))])\n",
    "    precision = tp / np.sum([yhats[i] == 1 for i in range(len(yhats))])\n",
    "    recall = tp / (np.sum([yhats[i] != dev_labels[i] and yhats[i] == 0 for i in range(len(yhats))]) + tp)\n",
    "    f1 = 2 * (precision * recall) / (precision + recall)\n",
    "    return accuracy, f1, precision, recall\n",
    "\n",
    "\n",
    "def main(train_set,train_label,dev_set,dev_label,stemming,lower_case,laplace,pos_prior):\n",
    "    \n",
    "    predicted_labels = naiveBayes(train_set, train_labels, dev_set, laplace, pos_prior)\n",
    "\n",
    "    accuracy, f1, precision, recall = compute_accuracies(predicted_labels, dev_set, dev_labels)\n",
    "    print(\"Accuracy:\",accuracy)\n",
    "    print(\"F1-Score:\",f1)\n",
    "    print(\"Precision:\",precision)\n",
    "    print(\"Recall:\",recall)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "porter_stemmer = PorterStemmer()\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "bad_words = {'aed','oed','eed'} # these words fail in nltk stemmer algorithm\n",
    "def loadDir(name,stemming,lower_case):\n",
    "    # Loads the files in the folder and returns a list of lists of words from\n",
    "    # the text in each file\n",
    "    X0 = []\n",
    "    count = 0\n",
    "    for f in tqdm(listdir(name)):\n",
    "        fullname = name+f\n",
    "        text = []\n",
    "        with open(fullname, 'rb') as f:\n",
    "            for line in f:\n",
    "                if lower_case:\n",
    "                    line = line.decode(errors='ignore').lower()\n",
    "                    text += tokenizer.tokenize(line)\n",
    "                else:\n",
    "                    text += tokenizer.tokenize(line.decode(errors='ignore'))\n",
    "        if stemming:\n",
    "            for i in range(len(text)):\n",
    "                if text[i] in bad_words:\n",
    "                    continue\n",
    "                text[i] = porter_stemmer.stem(text[i])\n",
    "        X0.append(text)\n",
    "        count = count + 1\n",
    "    return X0\n",
    "\n",
    "def load_dataset(train_dir, dev_dir, stemming, lower_case):\n",
    "    X0 = loadDir(train_dir + '/pos/',stemming, lower_case)\n",
    "    X1 = loadDir(train_dir + '/neg/',stemming, lower_case)\n",
    "    X = X0 + X1\n",
    "    Y = len(X0) * [1] + len(X1) * [0]\n",
    "    Y = np.array(Y)\n",
    "\n",
    "    X_test0 = loadDir(dev_dir + '/pos/',stemming, lower_case)\n",
    "    X_test1 = loadDir(dev_dir + '/neg/',stemming, lower_case)\n",
    "    X_test = X_test0 + X_test1\n",
    "    Y_test = len(X_test0) * [1] + len(X_test1) * [0]\n",
    "    Y_test = np.array(Y_test)\n",
    "\n",
    "    return X,Y,X_test,Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
